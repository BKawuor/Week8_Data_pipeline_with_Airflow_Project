{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Background Information\n",
        "\n",
        "Our telecommunications company, MTN Rwanda, has a vast customer base, and we generate alarge amount of data daily. We must efficiently process and store this data to make informed business decisions. Therefore, we plan to develop a data pipeline to extract, transform, and load data from three CSV files and store it in a Postgres database.\n",
        "\n",
        "We require a skilled data\n",
        "engineer who can use the Airflow tool to develop the pipeline to achieve this.\n",
        "\n",
        "\n",
        "# Problem Statement\n",
        "\n",
        "The main challenge is that the data generated is in a raw format, and we need to process it efficiently to make it usable for analysis. This requires us to develop a data pipeline that can extract, transform and load the data from multiple CSV files into a single database, which can be used for further analysis.\n",
        "\n",
        "# Guidelines\n",
        "\n",
        "The data pipeline should be developed using Airflow, an open-source tool for creating and managing data pipelines. The following steps should be followed to develop the data pipeline:\n",
        "\n",
        "\n",
        "● The data engineer should start by creating a DAG (Directed Acyclic Graph) that definesthe workflow of the data pipeline.\n",
        "\n",
        "● The DAG should include tasks that extract data from the three CSV files.\n",
        "\n",
        "● After extraction, the data should be transformed using Python libraries to match therequired format.\n",
        "\n",
        "● Finally, the transformed data should be loaded into a Postgres database.\n",
        "\n",
        "● The data pipeline should be scheduled to run at a specific time daily using the Airflow scheduler.\n",
        "\n",
        "● We can use the shared file (mtnrwanda-dag.py) as a starting point.\n",
        "\n",
        "The following are sample CSV files that will be used in the data pipeline:\n",
        "\n",
        "● customer_data.csv\n",
        "\n",
        "● order_data.csv\n",
        "\n",
        "● payment_data.csv\n",
        "\n",
        "All files for this project can be downloaded from here (link).\n",
        "\n"
      ],
      "metadata": {
        "id": "ilpesYAUJaZY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsxES1v6JZNb"
      },
      "outputs": [],
      "source": [
        "#Install Iarflow\n",
        "!pip install apache-airflow\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import psycopg2\n",
        "import logging\n",
        "\n",
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DAG default arguments\n",
        "default_args = {\n",
        "    'owner': 'MTN Rwanda',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2023, 3, 19),\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5)\n",
        "}\n",
        "\n",
        "# DAG Definition\n",
        "dag = DAG('data_pipeline',\n",
        "          default_args=default_args,\n",
        "          schedule_interval=timedelta(days=1)\n",
        "          )\n",
        "\n",
        "# Data extraction from the CSV files\n",
        "def extract_data():\n",
        "    cust_df = pd.read_csv('customer_data.csv')\n",
        "    order_df = pd.read_csv('order_data.csv')\n",
        "    payment_df = pd.read_csv('payment_data.csv')\n",
        "\n",
        "    return cust_df, order_df, payment_df\n",
        "\n",
        "# Transform the data\n",
        "def transform_data(cust_df, order_df, payment_df):\n",
        "    # convert date of birth t datetome format\n",
        "    cust_df['date_of_birth'] = pd.to_datetime(cust_df['date_of_birth'])\n",
        "\n",
        "    #  customer order dataframes on  customer_id column merging\n",
        "    customer_order_df = pd.merge(cust_df, order_df, on='customer_id')\n",
        "\n",
        "    # payment dataframe with  merged dataframe on the order_id and customer_id columns\n",
        "    customer_payment_df = pd.merge(customer_order_df, payment_df, on=['order_id', 'customer_id'])\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    customer_payment_df.drop(columns=['customer_id', 'order_id'], inplace=True)\n",
        "\n",
        "    # Group the data by customer and aggregate the amount paid\n",
        "    customer_grouped_df = customer_payment_df.groupby(['first_name', 'last_name', 'email', 'country', 'gender', 'date_of_birth'])['amount'].sum().reset_index()\n",
        "\n",
        "    # Calculate the total value of orders made by each customer in a new column\n",
        "    customer_grouped_df['total_order_value'] = customer_payment_df.groupby(['first_name', 'last_name', 'email', 'country', 'gender', 'date_of_birth'])['price'].sum().values\n",
        "\n",
        "    # Calculate the customer lifetime value using the formula CLV = (average order value) x (number of orders made per year) x (average customer lifespan)\n",
        "    customer_grouped_df['average_order_value'] = customer_grouped_df['total_order_value'] / customer_grouped_df['amount']\n",
        "    customer_grouped_df['number_of_orders_per_year'] = customer_grouped_df['amount'] / ((pd.to_datetime('now') - customer_grouped_df['date_of_birth']).dt.days / 365)\n",
        "    customer_grouped_df['average_customer_lifespan'] = (pd.to_datetime('now') - customer_grouped_df['date_of_birth']).dt.days / 365\n",
        "    customer_grouped_df['clv'] = customer_grouped_df['average_order_value'] * customer_grouped_df['number_of_orders_per_year'] * customer_grouped_df['average_customer_lifespan']\n",
        "\n",
        "    return customer_grouped_df"
      ],
      "metadata": {
        "id": "fud66D8vKo3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load transformed data into a PostgreSQL db\n",
        "def load_data(transformed_df):\n",
        "    try:\n",
        "        # Connect to the PostgreSQL database\n",
        "        conn = psycopg2.connect(\n",
        "            host = \"34.170.193.146\"\n",
        "            database = \"MTNRwanda\"\n",
        "            user = \"root\"\n",
        "            password = \"root@123\"\n",
        "        )"
      ],
      "metadata": {
        "id": "gb0fSPU6Kvzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Open a cursor to perform database operations\n",
        "        cur = conn.cursor()\n",
        "\n",
        "        # Create the cust_ltv table\n",
        "        cur.execute(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS cust_ltv (\n",
        "                customer_id INTEGER PRIMARY KEY,\n",
        "                total_orders INTEGER,\n",
        "                total_amount NUMERIC(10,2),\n",
        "                avg_order_value NUMERIC(10,2),\n",
        "                ltv NUMERIC(10,2)\n",
        "            )\n",
        "        \"\"\")\n"
      ],
      "metadata": {
        "id": "FG2qhqq_Kwmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Insert the transformed data into the cust_ltv table\n",
        "        for index, row in transformed_df.iterrows():\n",
        "            cur.execute(\"\"\"\n",
        "                INSERT INTO cust_ltv (customer_id, total_orders, total_amount, avg_order_value, ltv)\n",
        "                VALUES (%s, %s, %s, %s, %s)\n",
        "            \"\"\", (row['customer_id'], row['total_orders'], row['total_amount'], row['avg_order_value'], row['ltv']))\n",
        "\n",
        "        conn.commit()"
      ],
      "metadata": {
        "id": "7ZzX_xKYKzYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Close the cursor and connection\n",
        "        cur.close()\n",
        "        conn.close()\n",
        "\n",
        "        # success message\n",
        "        logging.info(\"Data loaded successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # error message\n",
        "        logging.error(f\"Error loading data: {str(e)}\")\n",
        "        raise e"
      ],
      "metadata": {
        "id": "rQFDKt5lK3NG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract data task\n",
        "extract_data_task = PythonOperator(\n",
        "    task_id='extract_data',\n",
        "    python_callable=extract_data,\n",
        "    dag=dag\n",
        ")"
      ],
      "metadata": {
        "id": "s-u0FZb_K7xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform data task\n",
        "transform_data_task = PythonOperator(\n",
        "    task_id='transform_data',\n",
        "    python_callable=transform_data,\n",
        "    dag=dag\n",
        ")"
      ],
      "metadata": {
        "id": "qKk-4X2UK-yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data task\n",
        "load_data_task = PythonOperator(\n",
        "    task_id='load_data',\n",
        "    python_callable=load_data,\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "# task dependencies\n",
        "extract_data_task >> transform_data_task >> load_data_task"
      ],
      "metadata": {
        "id": "jPI6k7O7LB5g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}